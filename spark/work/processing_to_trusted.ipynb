{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aaa180c-faf5-4f2d-980b-c7ed7c051ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Cria um logger com o nome 'jupyter_logger'\n",
    "logger = logging.getLogger('jupyter_logger')\n",
    "\n",
    "# Define o nível de log para DEBUG, então todas as mensagens de log serão mostradas\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Cria um manipulador de log que escreve as mensagens de log na saída padrão\n",
    "handler = logging.StreamHandler()\n",
    "\n",
    "# Define o nível de log do manipulador para DEBUG\n",
    "handler.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefd125a-23e0-4573-be5c-4e18d0552fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Merge with Delta Lake\") \\\n",
    "    .config(\"spark.task.maxFailures\", \"1\") \\\n",
    "    .config(\"fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"fs.s3a.access.key\", \"348GTMvf6HFATtNh\") \\\n",
    "    .config(\"fs.s3a.secret.key\", \"nqyaOwy8bOSF8OuIeY8urJiHYVFQWqpx\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def executing_merge(raw_bucket,trust_bucket, spark=spark):\n",
    "    \n",
    "    \n",
    "    # raw_bucket = \"s3a://raw/user\"\n",
    "\n",
    "    newData = spark.read.json(f\"{raw_bucket}/*\")\n",
    "    path = trust_bucket #\"s3a://trusted/user/\"\n",
    "    \n",
    "    \n",
    "    df_raw = spark.read.format(\"binaryFile\").load(f\"{raw_bucket}/*\")\n",
    "    count_raw = df_raw.count()\n",
    "    \n",
    "    \n",
    "    df_trusted_before_merge = spark.read.format(\"binaryFile\").load(f\"{path}/*\")\n",
    "    count_trusted_before_merge = df_trusted_before_merge.count()\n",
    "    \n",
    "    logger.debug(f\"Before merge in raw bucket has {count_raw} and trust bucket has {count_trusted_before_merge}\")\n",
    "    \n",
    "    \n",
    "    if DeltaTable.isDeltaTable(spark, path):\n",
    "        deltaTable = DeltaTable.forPath(spark, path)\n",
    "    else:\n",
    "        \n",
    "        selectedData = newData.select(col('data.id').alias('id'),col('data.first_name').alias('first_name'),col('data.last_name').alias('last_name'),col('data.email').alias('email'),col('data.date_of_birth').alias('date_of_birth'))\n",
    "        selectedData.write.format(\"delta\").save(path)\n",
    "        deltaTable = DeltaTable.forPath(spark, path)\n",
    "    \n",
    "    logger.debug(\"Executing merge\")\n",
    "    deltaTable.alias(\"oldData\") \\\n",
    "        .merge(\n",
    "            newData.alias(\"newData\"),\n",
    "            \"oldData.id = newData.data.id\") \\\n",
    "        .whenMatchedUpdate(set = { \n",
    "            \"first_name\" : \"newData.data.first_name\",\n",
    "            \"last_name\" : \"newData.data.last_name\",\n",
    "            \"email\" : \"newData.data.email\",\n",
    "            \"date_of_birth\" : \"newData.data.date_of_birth\" }) \\\n",
    "        .whenNotMatchedInsert(values = { \n",
    "            \"id\": \"newData.data.id\",\n",
    "            \"first_name\" : \"newData.data.first_name\",\n",
    "            \"last_name\" : \"newData.data.last_name\",\n",
    "            \"email\" : \"newData.data.email\",\n",
    "            \"date_of_birth\" : \"newData.data.date_of_birth\" }) \\\n",
    "        .execute()\n",
    "    \n",
    "    df_trusted_after_merge = spark.read.format(\"binaryFile\").load(f\"{path}/*\")\n",
    "    count_trusted_after_merge = df_trusted_after_merge.count()\n",
    "    logger.debug(f\"After merge in raw bucket has {count_raw} and trust bucket has {count_trusted_after_merge}\")\n",
    "    \n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b169a4b-a192-4e48-831b-fd8c29d97f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bucket=\"s3a://raw/user\"\n",
    "trust_bucket=\"s3a://trusted/user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66bae8d-5615-46f2-902c-251383a394a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Before merge in raw bucket has 126 and trust bucket has 36\n",
      "Executing merge\n",
      "After merge in raw bucket has 126 and trust bucket has 38\n"
     ]
    }
   ],
   "source": [
    "executing_merge(raw_bucket=raw_bucket,trust_bucket=trust_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec6bc6-db72-4b89-a66a-5da4f4b26a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
