{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aaa180c-faf5-4f2d-980b-c7ed7c051ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Cria um logger com o nome 'jupyter_logger'\n",
    "logger = logging.getLogger('jupyter_logger')\n",
    "\n",
    "# Define o nível de log para DEBUG, então todas as mensagens de log serão mostradas\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Cria um manipulador de log que escreve as mensagens de log na saída padrão\n",
    "handler = logging.StreamHandler()\n",
    "\n",
    "# Define o nível de log do manipulador para DEBUG\n",
    "handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Cria um formatador de log que adiciona a data e hora à mensagem de log\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Adiciona o formatador ao manipulador\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Adiciona o manipulador ao logger\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fefd125a-23e0-4573-be5c-4e18d0552fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# \"file:////home/user/to_process/006790cd-5c1b-47b8-9666-045f9dad6846.json\"\n",
    "# Crie uma sessão Spark\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Merge with Delta Lake\") \\\n",
    "    .config(\"spark.task.maxFailures\", \"1\") \\\n",
    "    .config(\"fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"fs.s3a.access.key\", \"TJeJFQPmgzTkMU2j\") \\\n",
    "    .config(\"fs.s3a.secret.key\", \"XgRcjxglL8EKI2NuTkNNsw106Hm48mVF\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def executing_merge(raw_bucket,trust_bucket, spark=spark):\n",
    "    \n",
    "    \n",
    "    # raw_bucket = \"s3a://raw/user\"\n",
    "\n",
    "    newData = spark.read.json(f\"{raw_bucket}/*\")\n",
    "\n",
    "    # Verifique se a tabela Delta existe trusted\n",
    "    path = trust_bucket #\"s3a://trusted/user/\"\n",
    "    \n",
    "    if DeltaTable.isDeltaTable(spark, path):\n",
    "        deltaTable = DeltaTable.forPath(spark, path)\n",
    "    else:\n",
    "        # Se a tabela Delta não existir, crie-a\n",
    "        selectedData = newData.select(col('data.id').alias('id'),col('data.first_name').alias('first_name'),col('data.last_name').alias('last_name'),col('data.email').alias('email'),col('data.date_of_birth').alias('date_of_birth'))\n",
    "        selectedData.write.format(\"delta\").save(path)\n",
    "        deltaTable = DeltaTable.forPath(spark, path)\n",
    "    # newData.printSchema()\n",
    "    # deltaTable.printSchema()\n",
    "    # Realize a operação de merge\n",
    "    logger.debug(\"Executing merge\")\n",
    "    deltaTable.alias(\"oldData\") \\\n",
    "        .merge(\n",
    "            newData.alias(\"newData\"),\n",
    "            \"oldData.id = newData.data.id\") \\\n",
    "        .whenMatchedUpdate(set = { \n",
    "            \"first_name\" : \"newData.data.first_name\",\n",
    "            \"last_name\" : \"newData.data.last_name\",\n",
    "            \"email\" : \"newData.data.email\",\n",
    "            \"date_of_birth\" : \"newData.data.date_of_birth\" }) \\\n",
    "        .whenNotMatchedInsert(values = { \n",
    "            \"id\": \"newData.data.id\",\n",
    "            \"first_name\" : \"newData.data.first_name\",\n",
    "            \"last_name\" : \"newData.data.last_name\",\n",
    "            \"email\" : \"newData.data.email\",\n",
    "            \"date_of_birth\" : \"newData.data.date_of_birth\" }) \\\n",
    "        .execute()\n",
    "    \n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b169a4b-a192-4e48-831b-fd8c29d97f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bucket=\"s3a://raw/user\"\n",
    "trust_bucket=\"s3a://trusted/user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0104908-ddae-4ffe-919a-d080d1eed8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 02:24:56,366 - jupyter_logger - DEBUG - Before merge in raw bucket has 38 and trust bucket has 15\n",
      "2024-05-20 02:24:56,366 - jupyter_logger - DEBUG - Before merge in raw bucket has 38 and trust bucket has 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "files_raw_before_merge = sc.wholeTextFiles(raw_bucket)\n",
    "file_count_raw_before_merge = files_raw_before_merge.count()\n",
    "    \n",
    "files_trusted_before_merge = sc.wholeTextFiles(trust_bucket)\n",
    "file_count_trusted_before_merge = files_trusted_before_merge.count()\n",
    "    \n",
    "logger.debug(f\"Before merge in raw bucket has {file_count_raw_before_merge} and trust bucket has {file_count_trusted_before_merge}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a66bae8d-5615-46f2-902c-251383a394a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 02:24:57,230 - jupyter_logger - DEBUG - Executing merge\n",
      "2024-05-20 02:24:57,230 - jupyter_logger - DEBUG - Executing merge\n"
     ]
    }
   ],
   "source": [
    "executing_merge(raw_bucket=raw_bucket,trust_bucket=trust_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86321b0d-0c6b-4ff5-9418-ea26ab02db6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 02:24:58,766 - jupyter_logger - DEBUG - After merge in raw bucket has 38 and trust bucket has 16\n",
      "2024-05-20 02:24:58,766 - jupyter_logger - DEBUG - After merge in raw bucket has 38 and trust bucket has 16\n"
     ]
    }
   ],
   "source": [
    "files_trusted_after_merge = sc.wholeTextFiles(trust_bucket)\n",
    "file_count_trusted_after_merge = files_trusted_after_merge.count()\n",
    "    \n",
    "logger.debug(f\"After merge in raw bucket has {file_count_raw_before_merge} and trust bucket has {file_count_trusted_after_merge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec6bc6-db72-4b89-a66a-5da4f4b26a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
